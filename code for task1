import os
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
import faiss
from transformers import pipeline

# Step 1: Extract Text from PDF
def extract_text_from_pdf(pdf_path):
    """Extract text from a PDF file."""
    text = ""
    reader = PdfReader(pdf_path)
    for page in reader.pages:
        text += page.extract_text() + "\n"
    return text

# Step 2: Chunk Text
def chunk_text(text, chunk_size=500):
    """Split text into manageable chunks."""
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

# Step 3: Embed Chunks and Create Vector Database
def create_vector_database(chunks, embedding_model='all-MiniLM-L6-v2'):
    """Embed text chunks and store them in a FAISS vector database."""
    # Load SentenceTransformer model
    model = SentenceTransformer(embedding_model)
    embeddings = model.encode(chunks)
    
    # Initialize FAISS index
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    
    return index, model, embeddings

# Step 4: Query the Vector Database
def query_vector_database(query, chunks, index, model, top_k=5):
    """Retrieve the most relevant chunks for a query."""
    query_embedding = model.encode([query])
    distances, indices = index.search(query_embedding, top_k)
    return [(chunks[i], distances[0][j]) for j, i in enumerate(indices[0])]

# Step 5: Generate Responses using LLM
def generate_response(retrieved_chunks, query, llm_model='t5-small'):
    """Generate a response using an LLM based on retrieved chunks."""
    summarizer = pipeline("summarization", model="t5-small", tokenizer="t5-small", device=-1)
    context = " ".join([chunk for chunk, _ in retrieved_chunks])
    prompt = f"Context: {context}\n\nQuestion: {query}\nAnswer only the relevant information:"
    
    # Summarize or refine response
    try:
        response = summarizer(prompt, max_length=150, min_length=50, do_sample=False)
        return response[0]['summary_text']
    except Exception as e:
        print(f"Error generating response: {e}")
        return "Could not generate a response."

# Step 6: Extract Tabular Data
def extract_table_from_page(pdf_path, page_num):
    """Extract tabular data from a specific PDF page."""
    reader = PdfReader(pdf_path)
    page = reader.pages[page_num]
    text = page.extract_text()
    
    # Parse the text assuming a table-like structure
    lines = text.split("\n")
    table_data = [line.split() for line in lines if line.strip()]
    return table_data

# Full Workflow
def main():
    # File Path
    pdf_path = "sample.pdf"  # Replace with the actual PDF path
    
    # Step 1: Extract text from PDF
    print("Extracting text from PDF...")
    text = extract_text_from_pdf(pdf_path)
    
    # Step 2: Chunk the extracted text
    print("Chunking text...")
    chunks = chunk_text(text)
    
    # Step 3: Create vector database
    print("Creating vector database...")
    index, embedding_model, embeddings = create_vector_database(chunks)
    
    # Step 4: Query the database
    query = "What is the unemployment rate for bachelor's degrees?"
    print("Querying the vector database...")
    retrieved_chunks = query_vector_database(query, chunks, index, embedding_model)
    
    # Print retrieved chunks for debugging
    print("Retrieved Chunks:")
    for chunk, distance in retrieved_chunks:
        print(f"Chunk: {chunk}\nDistance: {distance}\n")
    
    # Step 5: Generate a response using an LLM
    print("Generating response...")
    response = generate_response(retrieved_chunks, query)
    print("Response:")
    print(response)
    
    # Step 6: Extract tabular data from page 6
    page_num = 5  # Adjust for zero-based indexing
    print(f"Extracting tabular data from page {page_num + 1}...")
    table_data = extract_table_from_page(pdf_path, page_num)
    print("Tabular Data:")
    for row in table_data:
        print(row)

# Run the pipeline
if __name__ == "__main__":
    main()
